{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이름: 이선호\n",
    "\n",
    "학번: 20195214\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final project 규칙\n",
    "\n",
    "- 학생들 간의 질문은 금지합니다 (교수 및 조교에게 질문하세요)\n",
    "- 대 원칙은 \"내가 이해한 것만 활용하고 작성한다\" 입니다\n",
    "- 사용하는 기법들과 코드는 모두 보고서에 설명되어야하며, 설명이 없는 경우 감점이나 0점 처리합니다\n",
    "    - 예를 들어서 검색한 끝에 model ensemble이라는 기법이 유용할 것 같아서, 참고하여 성능을 개선한 경우\n",
    "        1. 참고자료의 출처를 작성할 것\n",
    "        2. 기반 이론을 설명할 것\n",
    "        3. 사용한 코드 분석 및 기법을 설명할 것\n",
    "- 주석이 없는 경우 채점하지 않습니다\n",
    "- Pretrained network은 사용하지 않습니다. 본 프로젝트에서는, random 초기화한 parameter들을 직접 학습하는 과정만 허용합니다\n",
    "    - 랜덤 초기화 기법은 특별한 제약이 없습니다\n",
    "- Competition 순위와 보고서는 각각 5:5로 점수가 반영됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project \n",
    "\n",
    "## Project: 새 품종 분류하기\n",
    "\n",
    "## Due date: 2021.06.15\n",
    "\n",
    "https://www.kaggle.com/c/detect-the-bird/leaderboard\n",
    "---\n",
    "\n",
    "* 아래 여러 셀에서 코드를 완성하는 부분을 수행하고, 보고서에 설명을 최대한 자세하게 적어주세요. 기준은 본인이 이해하고 있다는 것을 표현할 수 있는 부분을 모두 적으시면 됩니다.\n",
    "  \n",
    "  \n",
    "> **제출방법**: \n",
    "* 보고서에는 코드 캡쳐 첨부이외에도, 각 코드를 작성하는 기반 이론, 방법론과 설명을 작성하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목표: 앱을 위한 인공지능 알고리즘 개발 \n",
    "* 본 보고서에서는 모바일/웹앱을 위한 인공지능 알고리즘을 개발하라는 업무를 부여받았다고 가정합니다\n",
    "* 프로젝트가 완성본은, 사용자가 제공하는 image를 받아서 새의 종(種)을 예측합니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import library\n",
    "\n",
    "- 필요하다 생각되는 라이브러리를 미리 import해 놓은 항목입니다.(참고)\n",
    "- 필요 없는 라이브러리를 제거하거나 필요한 라이브러리를 추가하셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "#import helper\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드작성\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.RandomRotation(10),\n",
    "                                transforms.RandomResizedCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                                    [0.5, 0.5, 0.5])\n",
    "                                ])\n",
    "#대부분 사진 가운데 새가 배치됨, 224사이즈로 만들기, 테두리 cut, Normalize(0.5), Tensor화 진행\n",
    "trainset = datasets.ImageFolder('../input/detectthebird/imgs/train', transform=transform)\n",
    "#ImageFolder를 이용한 dataset준비, trainform을 통한 데이터 전처리\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Dataset 에 대한 Data Loaders 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드작성\n",
    "valid_size = 0.2  #Dataset을 Train set과 Validation set으로 나눌 비율 설정\n",
    "batch_size = 64   #Data Loader 구성시 data set의 batch size설정\n",
    "num_workers =0    # 0개의 subprocesses 가종\n",
    "\n",
    "num_train = len(trainset)  #trainset전체의 길이(sample 개수)\n",
    "num_train = int(num_train) #trainset sample 개수 정수화\n",
    "indices = list(range(num_train)) # 0 ~ sample 개수 -1까지 원소로 갖는 리스트 생성\n",
    "np.random.shuffle(indices) #리스트 indices내의 원소를 랜덤 셔플\n",
    "split = int(np.floor(valid_size * num_train)) #dataset을 20%비율로 스플릿 해줄 index 계산\n",
    "train_idx, valid_idx = indices[split:], indices[:split] \n",
    "#계산한 index에 따라 80%와 20%로 dataset을 train set과 validation set으로 나눔\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx) #랜덤 리스트 생성\n",
    "valid_sampler = SubsetRandomSampler(valid_idx) #랜덤 리스트 생성\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "#설정한 dataset, batch size, sampler, num_workers값에 따라 train loader 생성\n",
    "valid_loader = DataLoader(trainset, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "#설정한 dataset, batch size, sampler, num_workers값에 따라 vaild loader 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Neural Network 생성\n",
    "- Pretrained model을 허용하지 않습니다. (직접 모델을 설계해 주세요)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드작성\n",
    "#코드작성\n",
    "from torch import nn, optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module): #nn.Module 상속\n",
    "    def __init__(self):\n",
    "        super().__init__() #nn.Module.__init__() 실행\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 10, 8, padding=1) \n",
    "        # kernel = 8x8, 채널 수 10인 Convolution Network\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        #Pooling Layer - MaxPooling (2x2) stride =2\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Conv2d(10, 20, 3, padding=1)\n",
    "        # kernel = 3x3, 채널 수 20인 Convolution Network\n",
    "\n",
    "        self.conv3 = nn.Conv2d(20, 40, 3, padding=1)\n",
    "        # kernel = 3x3, 채널 수 40인 Convolution Network\n",
    "\n",
    "        self.conv4 = nn.Conv2d(40, 20, 3, padding=1)\n",
    "        # kernel = 3x3, 채널 수 20인 Convolution Network\n",
    "        \n",
    "        self.fc1 = nn.Linear(14580, 7290)\n",
    "        #input dimension : 14580 output diemsion : 7290인 Linear Regression model\n",
    "        \n",
    "        self.fc2 = nn.Linear(7290, 3145)\n",
    "        #input dimension : 7290 output diemsion : 3145인 Linear Regression model\n",
    "        \n",
    "        self.fc3 = nn.Linear(3145, 15)\n",
    "        #input dimension : 3145 output diemsion : 15인 Linear Regression model\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        #dropout layer\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Convolution layer -> Relu -> Maxpool layer적용\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #Convolution layer -> Relu -> Maxpool layer적용\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #Convolution layer -> Relu -> Maxpool layer적용\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        #Convolution layer -> Relu 적용\n",
    "        x = F.relu(self.conv4(x))\n",
    "        #image input을 평탄화\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #Linear layer -> Relu 적용\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #dropout layer 적용\n",
    "        x = self.dropout(x)\n",
    "        #Linear layer -> Relu 적용\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #dropout 적용\n",
    "        x = self.dropout(x)\n",
    "        #Linear layer -> Relu\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "#모델 객체 생성\n",
    "model = Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 4: Cost (Loss) Function 과 Optimizer 선택\n",
    " \n",
    " [loss function](http://pytorch.org/docs/stable/nn.html#loss-functions) 및 [optimizer](http://pytorch.org/docs/stable/optim.html)를 선택하여 코드를 완성하세요.\n",
    " \n",
    " 위 링크에서 다양한 Loss Function과 Optimize Function을 확인 할 수 있습니다\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function 선택 - CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer 선택 - SGD, learning rate : 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "#Softmax\n",
    "softmax = nn.Softmax(dim=1)\n",
    "#가장 높은 확률 값을 리턴해주는 함수 get_pred생성\n",
    "def get_pred(ps):\n",
    "    return torch.argmax(ps, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 구성한 모델에 대한 Train and Validate 진행\n",
    "\n",
    "* 코드 전체를 주석으로 설명하세요\n",
    "* Epoch 별로 Loss나 Accuracy를 출력하여 학습 진행 과정을 확인 할 수 있도록 합니다\n",
    "* 출력 예시는 주어지나 정해진 형식은 없습니다\n",
    "* 최적의 모델 저장\n",
    "\n",
    "예제:\n",
    "```\n",
    "Started Training...\n",
    "Epoch: 1 \tTraining Loss: 3.317162 \tValidation Loss: 4.162958\n",
    "Epoch: 2 \tTraining Loss: 2.420140 \tValidation Loss: 4.182362\n",
    "...\n",
    "...\n",
    "Finished training\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs 설정\n",
    "epochs = 500\n",
    "#train set의 loss를 저장할 train_loss\n",
    "train_loss = torch.zeros(epochs)\n",
    "#validation set의 loss를 저장할 val_loss\n",
    "val_loss = torch.zeros(epochs)\n",
    "#train set의 예측 성공률을 저장할 train_acc\n",
    "train_acc = torch.zeros(epochs)\n",
    "#validation set의 예측 성공률을 저장할 val_acc\n",
    "val_acc = torch.zeros(epochs) \n",
    "\n",
    "\n",
    "for e in range(epochs): #epochs 수 만큼 반복\n",
    "    print(f\"------ Starting epoch: {e} --------\")\n",
    "\n",
    "    \n",
    "    ##########Train set##########\n",
    "    \n",
    "    for inputs, labels in train_loader: #train set에 있는 image들과 label을 순서대로\n",
    "       \n",
    "        #모델에 image 입력\n",
    "        logits = model(inputs)\n",
    "        #loss 계산\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        #optimizer 초기화  -> gradients값을 0으로 초기화   \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #loss에 대한 미분 계산 (backpropagation)\n",
    "        loss.backward()\n",
    "        \n",
    "        #optimizer 진행 Stochastic gradient descent(SGD) \n",
    "        optimizer.step()\n",
    "\n",
    "        #train_loss에 loss값 저장\n",
    "        train_loss[e] += loss.item()\n",
    "        #softmax함수를 통해 확률값으로 변환\n",
    "        ps = softmax(logits)\n",
    "        #확률값 중 가장 큰 값 저장\n",
    "        pred = get_pred(ps)\n",
    "        #예측과 결과가 같은 것들의 평균 계산 - 정확도 계산\n",
    "        train_acc[e] += torch.sum(pred==labels)/labels.shape[0] \n",
    "    #epoch당 loss계산\n",
    "    train_loss[e] /= len(train_loader)\n",
    "    #epoch당 예측 정확도계산\n",
    "    train_acc[e] /= len(train_loader)  \n",
    "    \n",
    "    ############Vaildaion set###############\n",
    "    \n",
    "    #gradient 계산 중지\n",
    "    with torch.no_grad():\n",
    "        #evalutation mode (dropout 비활성화)\n",
    "        model.eval()\n",
    "        for inputs, labels in valid_loader:\n",
    "            \n",
    "            #모델에 image 입력\n",
    "            logits = model(inputs)\n",
    "            #loss 계산\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            #train_loss에 loss값 저장\n",
    "            val_loss[e] += loss.item()\n",
    "            #softmax함수를 통해 확률값으로 변환            \n",
    "            ps = softmax(logits)\n",
    "            #확률값 중 가장 큰 값 저장            \n",
    "            pred = get_pred(ps)\n",
    "            #예측과 결과가 같은 것들의 평균 계산 - 정확도 계산\n",
    "            val_acc[e] += torch.sum(pred==labels)/labels.shape[0] \n",
    "\n",
    "        #epoch당 loss계산\n",
    "        val_loss[e] /= len(valid_loader)\n",
    "        #epoch당 예측 정확도계산\n",
    "        val_acc[e] /= len(valid_loader) \n",
    "    \n",
    "     # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        e, train_loss[e], val_loss[e]))\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining accuracy: {:.6f} \\tValidation accuracy: {:.6f}'.format(\n",
    "        e, train_acc[e], val_acc[e]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: CNN model training/validation 분석\n",
    "   * tranining loss와 validation loss 그래프를 통해서 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loss 그래프 출력\n",
    "plt.plot(train_loss)\n",
    "#validation loss 그래프 출력\n",
    "plt.plot(val_loss)\n",
    "#그래프에 십자선\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 최적의 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train 예측정확도 그래프 출력\n",
    "plt.plot(train_acc)\n",
    "#validation 예측정확도 그래프 출력\n",
    "plt.plot(val_acc)\n",
    "#그래프에 십자선\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Predict with Test Data \n",
    "\n",
    "\n",
    "### 예시 코드를 제공해 드립니다. 필요한 부분을 채워 사용하시거나 직접 코드를 작성 하셔도 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self,imgpath,transform=None): #image data set 경로, tranform 입력\n",
    "        \n",
    "        self.imgpath = imgpath\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        #length of data set\n",
    "        return len(self.imgpath)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x = self.transform(Image.open(self.imgpath[idx]).convert('RGB'))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test dataset과 dataloader 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data 불러오기\n",
    "test_set = sorted(glob.glob('../input/detect-the-bird/imgs/test/*'))\n",
    "\n",
    "#test set을 클래스 dataset을 이용해 transform을 적용\n",
    "test_data = dataset(test_set,transform)\n",
    "#test_DataLoader 생성\n",
    "test_DataLoader = DataLoader(test_data, batch_size=16, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#가장 확률이 큰 레이블들을 임시저장할 리스트\n",
    "decoded = []\n",
    "#gradient 연산을 끄기 위한 함수\n",
    "with torch.no_grad(): \n",
    "\n",
    "\n",
    "\n",
    "    #test에는 레이블이 없고 차례대로 test사진을 뽑아준다.\n",
    "    for i,images in enumerate(test_DataLoader):\n",
    "\n",
    "        #test사진을 CUDA에 넣어준다.\n",
    "        images = images.to('cuda')\n",
    "        #학습 시킨 모델에 이미지 넣어주기\n",
    "        logits = model(images)\n",
    "        #softmax함수를 사용해서 확률을 뽑아준다.\n",
    "        ps = F.softmax(logits, dim=1)\n",
    "        #15개의 레이블에서 가장 확률이 큰 레이블을 가져온다.\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        #flatten data\n",
    "        top_class = top_class.data.view(-1)\n",
    "        #리스트에 예측한 레이블들을 저장\n",
    "        decoded.append(top_class)\n",
    "        print(top_class, top_class.shape)\n",
    "  \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs마다 tensor 타입으로 레이블들이 저장되기 때문에 이를 정수형으로 변환해서 리스트 pred에 저장해준다.\n",
    "pred = []\n",
    "for i in range(len(decoded)):\n",
    "    for j in range(len(decoded[i])):\n",
    "        pred.append(int(decoded[i][j]))\n",
    "        \n",
    "        \n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측 결과 인덱스를 저장한 pred 값을 사용해 클래스를 매칭하기 위한 class_name을 담아 놓은 리스트입니다.\n",
    "\n",
    "주의할 점은 Train과정에서 사용된 class label 값과 같은 순서로 저장이 되어 있어야 한다는 점입니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [i.split('/')[-1] for i in glob.glob('../input/detect-the-bird/imgs/train/*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=[]\n",
    "category =[]\n",
    "for i in range(len(test_set)):\n",
    "    id.append(test_set[i].split('/')[-1])\n",
    "    category.append(classes[pred[i]])\n",
    "pd.DataFrame({'Id':id,'Category':category}).to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
